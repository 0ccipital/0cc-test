# Ollama Context Tree Chat üå≥

Entirely generated by claude-sonnet-4-20250514 on 20250720 on beta.lmarena.ai

A powerful CLI tool that enables branching conversations with Ollama models, allowing you to revert to previous states and explore different conversation paths without context pollution.

## The Problem This Solves

Traditional LLM chats suffer from "context pollution" - when the model makes a mistake, that incorrect information remains in the conversation context even after correction. This tool solves that by allowing you to:

1. **Branch conversations** - Try different approaches from the same starting point
2. **Revert cleanly** - Go back to before a mistake occurred  
3. **Maintain context trees** - Visualize and navigate complex conversation flows

## Features

üå≥ **Conversation Branching** - Create multiple conversation paths from any point  
üîÑ **State Reversion** - Jump back to any previous conversation state with clean context  
üßπ **Context Pollution Prevention** - Avoid "mistake contamination" in your chats  
üè∑Ô∏è **Simple Tagging** - Mark states as ‚úÖ good-path, ‚ùå bad-path, ‚ö° branch-point  
üé® **Rich Terminal UI** - Beautiful, color-coded interface optimized for iTerm2/Terminal  
üìä **Visual Tree Display** - See your conversation structure at a glance  
üíæ **Persistent Conversations** - Save and load conversation trees  
üîç **Branch Comparison** - Compare different conversation paths side-by-side  
üöÄ **Fast Model Switching** - Change models without losing conversation state  
‚ö° **Quick Tagging** - Single-keystroke tagging during conversations  

## Installation

This project uses [uv](https://github.com/astral-sh/uv) for fast, reliable Python package management.

### Prerequisites

- Python 3.8+
- [Ollama](https://ollama.ai) installed and running
- [uv](https://github.com/astral-sh/uv) package manager

### Setup

1. **Install uv** (if you haven't already):
   ```bash
   # macOS/Linux
   curl -LsSf https://astral.sh/uv/install.sh | sh
   
   # Windows
   powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
   ```

2. **Clone the repository:**
   ```bash
   git clone https://github.com/yourusername/ollama-context-tree-chat.git
   cd ollama-context-tree-chat
   ```

3. **Initialize the project with uv:**
   ```bash
   uv init --python 3.8
   uv add requests
   ```

4. **Make sure Ollama is running:**
   ```bash
   ollama serve
   ```

5. **Install at least one model:**
   ```bash
   ollama pull llama3.2
   # or any other model you prefer
   ```

6. **Run the application:**
   ```bash
   uv run python main.py
   ```

### Optional: Create a shell alias

Add to your `.zshrc` or `.bashrc`:
```bash
alias octree="cd /path/to/ollama-context-tree-chat && uv run python main.py"
```

## Usage

### Starting a Chat

1. Run the application
2. Select a model from your installed Ollama models
3. Optionally set a system message
4. Start chatting with branching capabilities!

### Commands

| Command | Description |
|---------|-------------|
| `/help` | Show available commands and examples |
| `/states` | Display conversation tree and all states |
| `/revert` | Smart branch point selection and reversion |
| `/tag` | Tag current state (‚úÖ good, ‚ùå bad, ‚ö° branch-point) |
| `/compare` | Compare current state with sibling branches |
| `/save` | Save conversation tree to file |
| `/load` | Load conversation tree from file |
| `/model` | Change the current model |
| `/system` | Set or update system message |
| `/clear` | Clear the screen |
| `/quit` or `/q` | Exit the application |

### Quick Tagging

After each AI response, you can quickly tag the conversation:
- **`g`** - Mark as ‚úÖ good-path (this approach worked well)
- **`b`** - Mark as ‚ö° branch-point (good place to try alternatives)  
- **`x`** - Mark as ‚ùå bad-path (this approach didn't work)
- **`Enter`** - Skip tagging

### Example Workflow

```
1. Ask: "How do I optimize this Python function?"
   ‚Üí Creates state_1

2. AI suggests approach A (using list comprehension)
   ‚Üí Tag with 'g' (good approach) ‚úÖ
   ‚Üí Creates state_2

3. Ask: "What about memory usage?"
   ‚Üí AI explains memory implications
   ‚Üí Creates state_3

4. Realize you want to try a different approach
   ‚Üí Use /revert to go back to state_1

5. Ask: "What about using NumPy instead?"
   ‚Üí Creates state_4_a (new branch from state_1)
   ‚Üí Tag with 'b' (worth exploring) ‚ö°

Result: Clean exploration of different approaches without context pollution!
```

### Visual Tree Example

```
state_1: How do I optimize this Python function?
‚îú‚îÄ‚îÄ state_2: List comprehension approach ‚úÖ ‚Üí state_3: Memory usage
‚îî‚îÄ‚îÄ state_4_a: NumPy approach ‚ö° ‚Üê current
```

## Project Structure

```
ollama-context-tree-chat/
‚îú‚îÄ‚îÄ pyproject.toml          # uv project configuration
‚îú‚îÄ‚îÄ uv.lock                 # Dependency lock file
‚îú‚îÄ‚îÄ README.md               # This file
‚îú‚îÄ‚îÄ main.py                 # CLI interface and command handling
‚îú‚îÄ‚îÄ tree.py                 # Core tree operations and state management
‚îú‚îÄ‚îÄ ollama.py               # Ollama API client
‚îú‚îÄ‚îÄ display.py              # Enhanced visualization and UI formatting
‚îú‚îÄ‚îÄ tags.py                 # Simple tagging system
‚îî‚îÄ‚îÄ config.py               # Configuration settings
```

## Configuration

### pyproject.toml

```toml
[project]
name = "ollama-context-tree-chat"
version = "1.0.0"
description = "Branching conversation tool for Ollama models"
authors = [
    {name = "Your Name", email = "your.email@example.com"}
]
dependencies = [
    "requests>=2.31.0",
]
requires-python = ">=3.8"
readme = "README.md"
license = {text = "MIT"}

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.uv]
dev-dependencies = [
    "pytest>=7.0.0",
    "black>=23.0.0",
    "ruff>=0.1.0",
]
```

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `OLLAMA_BASE_URL` | `http://localhost:11434` | Ollama server URL |

### File Locations

- **Conversation history**: `~/.ollama_chat_history`
- **Saved conversations**: `~/.ollama_conversations/`
- **Auto-created on first run**

## Terminal Optimization

### For iTerm2 Users (Recommended)

1. **Font**: Use a Nerd Font for best emoji rendering
2. **Colors**: Enable "Use bright colors for bold text"
3. **Theme**: Dark themes work best with the color scheme
4. **Window title**: Automatically set to "Ollama Context Tree Chat"

### For Terminal.app Users

1. **Profile**: Use "Pro" or "Homebrew" theme
2. **Font**: SF Mono or Menlo work well
3. **Colors**: Ensure ANSI colors are enabled

### For Other Terminals

- Ensure ANSI color support is enabled
- Use a monospace font for proper tree alignment
- Minimum width of 80 characters recommended

## Development

### Running Tests

```bash
uv run pytest
```

### Code Formatting

```bash
uv run black *.py
uv run ruff check *.py
```

### Adding Dependencies

```bash
# Add runtime dependency
uv add package-name

# Add development dependency  
uv add --dev package-name
```

### Project Commands

```bash
# Install dependencies
uv sync

# Run the application
uv run python main.py

# Run with specific Python version
uv run --python 3.11 python main.py

# Create virtual environment
uv venv

# Activate virtual environment
source .venv/bin/activate  # Unix
# or
.venv\Scripts\activate     # Windows
```

## Troubleshooting

### Common Issues

**"Cannot connect to Ollama"**
- Ensure Ollama is running: `ollama serve`
- Check if models are installed: `ollama list`
- Verify Ollama is accessible: `curl http://localhost:11434/api/tags`

**"No models found"**
- Install a model: `ollama pull llama3.2`
- Check available models: `ollama list`
- Ensure Ollama service is running

**Terminal display issues**
- Ensure your terminal supports ANSI colors
- Try a different terminal theme
- Check terminal width is adequate (minimum 80 characters)
- Update to a recent version of your terminal

**Quick tagging not working**
- This feature uses `select()` which may not work on all systems
- Fallback: Use `/tag` command instead
- On Windows: Quick tagging automatically falls back to manual mode

**Save/Load issues**
- Check permissions for `~/.ollama_conversations/` directory
- Ensure sufficient disk space
- Verify JSON file format if loading fails

### Performance Tips

- Use smaller models for faster responses during development
- The tool automatically unloads models when reverting to save memory
- Command history is limited to 1000 entries for performance
- Large conversation trees (>100 states) may have slower rendering

## License

Unlicense - see LICENSE file for details.

## Support
- **Documentation**: This README and inline code documentation

# Changelog

## [1.0.0] - 2025-07-20

### Added
- **Core branching functionality** - Create multiple conversation paths from any point
- **State reversion** - Jump back to previous states with clean context (model unloading)
- **Simple tagging system** - Mark states as ‚úÖ good-path, ‚ùå bad-path, ‚ö° branch-point
- **Enhanced tree visualization** - Horizontal and vertical tree display modes
- **Smart navigation** - Branch point detection and intelligent revert suggestions
- **Conversation persistence** - Save and load conversation trees as JSON files
- **Branch comparison** - Side-by-side comparison of different conversation paths
- **Rich terminal UI** - Color-coded interface optimized for iTerm2 and Terminal.app
- **Quick tagging** - Single-keystroke tagging during conversations (g/b/x keys)
- **Model management** - Easy model switching without losing conversation state
- **Command history** - Persistent readline history across sessions
- **Auto-save directory** - Automatic creation of `~/.ollama_conversations/`

### Technical Features
- **Modular architecture** - Clean separation of concerns across 6 focused modules
- **Comprehensive error handling** - Graceful degradation and helpful error messages
- **Terminal optimization** - Dynamic width detection and text wrapping
- **Signal handling** - Graceful exit on Ctrl+C
- **Cross-platform support** - Works on macOS, Linux, and Windows
- **uv package management** - Modern Python dependency management

### Commands Implemented
- `/help` - Show available commands and examples
- `/states` - Display conversation tree and state summary
- `/revert` - Smart branch point selection and reversion
- `/tag` - Interactive tagging interface
- `/compare` - Branch comparison functionality
- `/save` - Save conversation trees with timestamp
- `/load` - Load saved conversations with file browser
- `/model` - Model selection and switching
- `/system` - System message configuration
- `/clear` - Screen clearing with header refresh
- `/quit`, `/q` - Graceful application exit

### UI/UX Features
- **Smart prompts** - Context-aware prompts showing current state and branch info
- **Progress indicators** - Visual feedback for long operations
- **Branch indicators** - Clear visual markers for branching points (üåø‚ö°)
- **Current state highlighting** - Always know where you are in the conversation tree
- **Path visualization** - Show full path from root to current state
- **Timestamp tracking** - All states include creation timestamps
- **Terminal title setting** - Automatic window title for iTerm2

### Configuration
- **Configurable settings** - Centralized configuration in `config.py`
- **Feature flags** - Enable/disable quick tagging and other features
- **Default directories** - Sensible defaults for history and save locations
- **Color customization** - Full ANSI color support with fallbacks

## [Unreleased]

### Planned Features
- **Web interface** - Browser-based UI for non-technical users
- **Multi-model comparison** - Compare responses from different models simultaneously
- **Conversation templates** - Reusable conversation patterns and workflows
- **Export formats** - Markdown, PDF, and other export options
- **Analytics** - Conversation quality metrics and pattern analysis
- **Collaboration** - Share and merge conversation trees
- **Plugin system** - Extensible architecture for custom features

### Known Issues
- Quick tagging timeout may not work on all terminal configurations
- Large conversation trees (>100 states) may have slower rendering
- Windows compatibility for some terminal features may be limited

---

## Version History

- **v1.0.0** - Initial release with full branching conversation functionality