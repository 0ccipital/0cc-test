# Ollama Context Tree Chat ðŸŒ³

Entirely generated by claude-sonnet-4-20250514 on 20250720 on beta.lmarena.ai

A powerful CLI tool that enables branching conversations with Ollama models, allowing you to revert to previous states and explore different conversation paths without context pollution.

## Features

ðŸŒ³ **Conversation Branching** - Create multiple conversation paths from any point  
ðŸ”„ **State Reversion** - Jump back to any previous conversation state  
ðŸ§¹ **Context Pollution Prevention** - Avoid "mistake contamination" in your chats  
ðŸŽ¨ **Rich Terminal UI** - Beautiful, color-coded interface optimized for iTerm2/Terminal  
ðŸ“Š **Visual Tree Display** - See your conversation structure at a glance  
ðŸ’¾ **Persistent History** - Command history saved across sessions  
ðŸš€ **Fast Model Switching** - Change models without losing conversation state  

## The Problem This Solves

Traditional LLM chats suffer from "context pollution" - when the model makes a mistake, that incorrect information remains in the conversation context even after correction. This tool solves that by allowing you to:

1. **Branch conversations** - Try different approaches from the same starting point
2. **Revert cleanly** - Go back to before a mistake occurred  
3. **Maintain context trees** - Visualize and navigate complex conversation flows

## Installation

This project uses [uv](https://github.com/astral-sh/uv) for fast, reliable Python package management.

### Prerequisites

- Python 3.8+
- [Ollama](https://ollama.ai) installed and running
- [uv](https://github.com/astral-sh/uv) package manager

### Setup

1. **Clone the repository:**
   ```bash
   git clone https://github.com/yourusername/ollama-context-tree-chat.git
   cd ollama-context-tree-chat
   ```

2. **Install dependencies with uv:**
   ```bash
   uv sync
   ```

3. **Make sure Ollama is running:**
   ```bash
   ollama serve
   ```

4. **Run the application:**
   ```bash
   uv run python ollama_chat_tree.py
   ```

### Optional: Create a shell alias

Add to your `.zshrc` or `.bashrc`:
```bash
alias octree="cd /path/to/ollama-context-tree-chat && uv run python ollama_chat_tree.py"
```

## Usage

### Starting a Chat

1. Run the application
2. Select a model from your installed Ollama models
3. Optionally set a system message
4. Start chatting!

### Commands

| Command | Description |
|---------|-------------|
| `/help` | Show available commands |
| `/states` | Display conversation tree and all states |
| `/revert` | Select and revert to a previous state |
| `/model` | Change the current model |
| `/system` | Set or update system message |
| `/clear` | Clear the screen |
| `/path` | Show path to current state |
| `/info` | Show detailed current state information |
| `/quit` or `/q` | Exit the application |

### Example Workflow

```
1. Ask: "What's the capital of France?"
   â†’ Creates state_1

2. Ask: "What's its population?"  
   â†’ Creates state_2 (continues from state_1)

3. Model gives wrong population
   â†’ Use /revert to go back to state_1

4. Ask: "What are its famous landmarks?"
   â†’ Creates state_3_a (new branch from state_1)

Result: Clean conversation without the wrong population data!
```

### Visual Tree Example

```
state_1: What's the capital of France?
â”œâ”€â”€ state_2: What's its population?
â””â”€â”€ state_3_a: What are its famous landmarks? ðŸŒ¿ â† CURRENT
```

## Project Structure

```
ollama-context-tree-chat/
â”œâ”€â”€ pyproject.toml          # uv project configuration
â”œâ”€â”€ uv.lock                 # Dependency lock file
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ ollama_chat_tree.py     # Main application
â””â”€â”€ .python-version         # Python version specification
```

## Configuration

### pyproject.toml

The project is configured with uv's modern Python packaging:

```toml
[project]
name = "ollama-context-tree-chat"
version = "0.1.0"
description = "Branching conversation tool for Ollama models"
dependencies = [
    "requests>=2.31.0",
]
requires-python = ">=3.8"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.uv]
dev-dependencies = [
    "pytest>=7.0.0",
    "black>=23.0.0",
    "ruff>=0.1.0",
]
```

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `OLLAMA_BASE_URL` | `http://localhost:11434` | Ollama server URL |

## Terminal Optimization

### For iTerm2 Users

1. **Font**: Use a Nerd Font for best emoji rendering
2. **Colors**: Enable "Use bright colors for bold text"
3. **Theme**: Dark themes work best with the color scheme

### For Terminal.app Users

1. **Profile**: Use "Pro" or "Homebrew" theme
2. **Font**: SF Mono or Menlo work well
3. **Colors**: Ensure ANSI colors are enabled

## Development

### Running Tests

```bash
uv run pytest
```

### Code Formatting

```bash
uv run black ollama_chat_tree.py
uv run ruff check ollama_chat_tree.py
```

### Adding Dependencies

```bash
# Add runtime dependency
uv add package-name

# Add development dependency  
uv add --dev package-name
```

## Troubleshooting

### Common Issues

**"Cannot connect to Ollama"**
- Ensure Ollama is running: `ollama serve`
- Check if models are installed: `ollama list`
- Verify Ollama is accessible: `curl http://localhost:11434/api/tags`

**"No models found"**
- Install a model: `ollama pull llama3.2`
- Check available models: `ollama list`

**Terminal display issues**
- Ensure your terminal supports ANSI colors
- Try a different terminal theme
- Check terminal width is adequate (minimum 80 characters)

### Performance Tips

- Use smaller models for faster responses during development
- The tool automatically unloads models when reverting to save memory
- Command history is limited to 1000 entries for performance

## Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make your changes
4. Run tests: `uv run pytest`
5. Format code: `uv run black . && uv run ruff check .`
6. Submit a pull request

## License

MIT License - see LICENSE file for details.

## Acknowledgments

- [Ollama](https://ollama.ai) for the excellent local LLM platform
- [uv](https://github.com/astral-sh/uv) for fast Python package management
- The Python community for excellent terminal libraries

---

Entirely generated by claude-sonnet-4-20250514 on 20250720 on beta.lmarena.ai
